{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.1\n",
      "Is GPU available? True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "# from torchsummary import summary\n",
    "import argparse\n",
    "import logging\n",
    "from utils import StandardScaler, DataLoader, masked_mae_loss, masked_mape_loss, masked_mse_loss, masked_rmse_loss\n",
    "from MegaCRN import MegaCRN\n",
    "print(torch.version.cuda)\n",
    "print('Is GPU available? {}\\n'.format(torch.cuda.is_available()))\n",
    "\n",
    "def print_model(model):\n",
    "    param_count = 0\n",
    "    logger.info('Trainable parameter list:')\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(name, param.shape, param.numel())\n",
    "            param_count += param.numel()\n",
    "    logger.info(f'In total: {param_count} trainable parameters.')\n",
    "    return\n",
    "\n",
    "def get_model():  \n",
    "    model = MegaCRN(num_nodes=args.num_nodes, input_dim=args.input_dim, output_dim=args.output_dim, horizon=args.horizon, \n",
    "                    rnn_units=args.rnn_units, num_layers=args.num_rnn_layers, mem_num=args.mem_num, mem_dim=args.mem_dim, \n",
    "                    cheb_k = args.max_diffusion_step, cl_decay_steps=args.cl_decay_steps, use_curriculum_learning=args.use_curriculum_learning).to(device)\n",
    "    return model\n",
    "\n",
    "def prepare_x_y(x, y):\n",
    "    \"\"\"\n",
    "    :param x: shape (batch_size, seq_len, num_sensor, input_dim)\n",
    "    :param y: shape (batch_size, horizon, num_sensor, input_dim)\n",
    "    :return1: x shape (seq_len, batch_size, num_sensor, input_dim)\n",
    "              y shape (horizon, batch_size, num_sensor, input_dim)\n",
    "    :return2: x: shape (seq_len, batch_size, num_sensor * input_dim)\n",
    "              y: shape (horizon, batch_size, num_sensor * output_dim)\n",
    "    \"\"\"\n",
    "    x0 = x[..., :args.input_dim]\n",
    "    y0 = y[..., :args.output_dim]\n",
    "    y1 = y[..., args.output_dim:]\n",
    "    x0 = torch.from_numpy(x0).float()\n",
    "    y0 = torch.from_numpy(y0).float()\n",
    "    y1 = torch.from_numpy(y1).float()\n",
    "    return x0.to(device), y0.to(device), y1.to(device) # x, y, y_cov\n",
    "    \n",
    "def evaluate(model, mode):\n",
    "    with torch.no_grad():\n",
    "        model = model.eval()\n",
    "        data_iter =  data[f'{mode}_loader'].get_iterator()\n",
    "        losses = []\n",
    "        ys_true, ys_pred = [], []\n",
    "        maes, mapes, mses = [], [], []\n",
    "        l_3, m_3, r_3 = [], [], []\n",
    "        l_6, m_6, r_6 = [], [], []\n",
    "        l_12, m_12, r_12 = [], [], []\n",
    "        for x, y in data_iter:\n",
    "            x, y, ycov = prepare_x_y(x, y)\n",
    "            output, h_att, query, pos, neg = model(x, ycov)\n",
    "            y_pred = scaler.inverse_transform(output)\n",
    "            y_true = scaler.inverse_transform(y)\n",
    "            loss1 = masked_mae_loss(y_pred, y_true) # masked_mae_loss(y_pred, y_true)\n",
    "            separate_loss = nn.TripletMarginLoss(margin=1.0)\n",
    "            compact_loss = nn.MSELoss()\n",
    "            loss2 = separate_loss(query, pos.detach(), neg.detach())\n",
    "            loss3 = compact_loss(query, pos.detach())\n",
    "            loss = loss1 + args.lamb * loss2 + args.lamb1 * loss3\n",
    "            losses.append(loss.item())\n",
    "            # Followed the DCRNN TensorFlow Implementation\n",
    "            maes.append(masked_mae_loss(y_pred, y_true).item())\n",
    "            mapes.append(masked_mape_loss(y_pred, y_true).item())\n",
    "            mses.append(masked_mse_loss(y_pred, y_true).item())\n",
    "            # Important for MegaCRN model to let T come first.\n",
    "            y_true, y_pred = y_true.permute(1, 0, 2, 3), y_pred.permute(1, 0, 2, 3)\n",
    "            l_3.append(masked_mae_loss(y_pred[2:3], y_true[2:3]).item())\n",
    "            m_3.append(masked_mape_loss(y_pred[2:3], y_true[2:3]).item())\n",
    "            r_3.append(masked_mse_loss(y_pred[2:3], y_true[2:3]).item())\n",
    "            l_6.append(masked_mae_loss(y_pred[5:6], y_true[5:6]).item())\n",
    "            m_6.append(masked_mape_loss(y_pred[5:6], y_true[5:6]).item())\n",
    "            r_6.append(masked_mse_loss(y_pred[5:6], y_true[5:6]).item())\n",
    "            l_12.append(masked_mae_loss(y_pred[11:12], y_true[11:12]).item())\n",
    "            m_12.append(masked_mape_loss(y_pred[11:12], y_true[11:12]).item())\n",
    "            r_12.append(masked_mse_loss(y_pred[11:12], y_true[11:12]).item())\n",
    "            ys_true.append(y_true)\n",
    "            ys_pred.append(y_pred)\n",
    "        mean_loss = np.mean(losses)\n",
    "        mean_mae, mean_mape, mean_rmse = np.mean(maes), np.mean(mapes), np.sqrt(np.mean(mses))\n",
    "        l_3, m_3, r_3 = np.mean(l_3), np.mean(m_3), np.sqrt(np.mean(r_3))\n",
    "        l_6, m_6, r_6 = np.mean(l_6), np.mean(m_6), np.sqrt(np.mean(r_6))\n",
    "        l_12, m_12, r_12 = np.mean(l_12), np.mean(m_12), np.sqrt(np.mean(r_12))\n",
    "        if mode == 'test':\n",
    "            logger.info('Horizon overall: mae: {:.4f}, mape: {:.4f}, rmse: {:.4f}'.format(mean_mae, mean_mape, mean_rmse))\n",
    "            logger.info('Horizon 15mins: mae: {:.4f}, mape: {:.4f}, rmse: {:.4f}'.format(l_3, m_3, r_3))\n",
    "            logger.info('Horizon 30mins: mae: {:.4f}, mape: {:.4f}, rmse: {:.4f}'.format(l_6, m_6, r_6))\n",
    "            logger.info('Horizon 60mins: mae: {:.4f}, mape: {:.4f}, rmse: {:.4f}'.format(l_12, m_12, r_12))\n",
    "        return mean_loss, ys_true, ys_pred\n",
    "        \n",
    "def traintest_model():  \n",
    "    model = get_model()\n",
    "    print_model(model)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, eps=args.epsilon)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=args.steps, gamma=args.lr_decay_ratio)\n",
    "    min_val_loss = float('inf')\n",
    "    wait = 0\n",
    "    batches_seen = 0\n",
    "    for epoch_num in range(args.epochs):\n",
    "        start_time = time.time()\n",
    "        model = model.train()\n",
    "        data_iter = data['train_loader'].get_iterator()\n",
    "        losses = []\n",
    "        for x, y in data_iter:\n",
    "            optimizer.zero_grad()\n",
    "            x, y, ycov = prepare_x_y(x, y)\n",
    "            output, h_att, query, pos, neg = model(x, ycov, y, batches_seen)\n",
    "            y_pred = scaler.inverse_transform(output)\n",
    "            y_true = scaler.inverse_transform(y)\n",
    "            loss1 = masked_mae_loss(y_pred, y_true) # masked_mae_loss(y_pred, y_true)\n",
    "            separate_loss = nn.TripletMarginLoss(margin=1.0)\n",
    "            compact_loss = nn.MSELoss()\n",
    "            loss2 = separate_loss(query, pos.detach(), neg.detach())\n",
    "            loss3 = compact_loss(query, pos.detach())\n",
    "            loss = loss1 + args.lamb * loss2 + args.lamb1 * loss3\n",
    "            losses.append(loss.item())\n",
    "            batches_seen += 1\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm) # gradient clipping - this does it in place\n",
    "            optimizer.step()\n",
    "        train_loss = np.mean(losses)\n",
    "        lr_scheduler.step()\n",
    "        val_loss, _, _ = evaluate(model, 'val')\n",
    "        # if (epoch_num % args.test_every_n_epochs) == args.test_every_n_epochs - 1:\n",
    "        end_time2 = time.time()\n",
    "        message = 'Epoch [{}/{}] ({}) train_loss: {:.4f}, val_loss: {:.4f}, lr: {:.6f}, {:.1f}s'.format(epoch_num + 1, \n",
    "                   args.epochs, batches_seen, train_loss, val_loss, optimizer.param_groups[0]['lr'], (end_time2 - start_time))\n",
    "        logger.info(message)\n",
    "        test_loss, _, _ = evaluate(model, 'test')\n",
    "        \n",
    "        if val_loss < min_val_loss:\n",
    "            wait = 0\n",
    "            min_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), modelpt_path)\n",
    "            # logger.info('Val loss decrease from {:.4f} to {:.4f}, saving model to pt'.format(min_val_loss, val_loss))\n",
    "        elif val_loss >= min_val_loss:\n",
    "            wait += 1\n",
    "            if wait == args.patience:\n",
    "                logger.info('Early stopping at epoch: %d' % epoch_num)\n",
    "                break\n",
    "    \n",
    "    logger.info('=' * 35 + 'Best model performance' + '=' * 35)\n",
    "    model = get_model()\n",
    "    model.load_state_dict(torch.load(modelpt_path))\n",
    "    test_loss, _, _ = evaluate(model, 'test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#########################################################################################    \n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dataset', type=str, choices=['METRLA', 'PEMSBAY', 'Flu_11_12', 'Flu_13_14'], default='Flu_11_12', help='which dataset to run')\n",
    "parser.add_argument('--trainval_ratio', type=float, default=0.8, help='the ratio of training and validation data among the total')\n",
    "parser.add_argument('--val_ratio', type=float, default=0.125, help='the ratio of validation data among the trainval ratio')\n",
    "parser.add_argument('--num_nodes', type=int, default=56, help='num_nodes')\n",
    "parser.add_argument('--seq_len', type=int, default=12, help='input sequence length')\n",
    "parser.add_argument('--horizon', type=int, default=12, help='output sequence length')\n",
    "parser.add_argument('--input_dim', type=int, default=1, help='number of input channel')\n",
    "parser.add_argument('--output_dim', type=int, default=1, help='number of output channel')\n",
    "parser.add_argument('--max_diffusion_step', type=int, default=3, help='max diffusion step or Cheb K')\n",
    "parser.add_argument('--num_rnn_layers', type=int, default=1, help='number of rnn layers')\n",
    "parser.add_argument('--rnn_units', type=int, default=64, help='number of rnn units')\n",
    "parser.add_argument('--mem_num', type=int, default=20, help='number of meta-nodes/prototypes')\n",
    "parser.add_argument('--mem_dim', type=int, default=64, help='dimension of meta-nodes/prototypes')\n",
    "parser.add_argument(\"--loss\", type=str, default='mask_mae_loss', help=\"mask_mae_loss\")\n",
    "parser.add_argument('--lamb', type=float, default=0.01, help='lamb value for separate loss')\n",
    "parser.add_argument('--lamb1', type=float, default=0.01, help='lamb1 value for compact loss')\n",
    "parser.add_argument(\"--epochs\", type=int, default=2, help=\"number of epochs of training\") #200\n",
    "parser.add_argument(\"--patience\", type=int, default=20, help=\"patience used for early stop\")\n",
    "parser.add_argument(\"--batch_size\", type=int, default=64, help=\"size of the batches\")\n",
    "parser.add_argument(\"--lr\", type=float, default=0.01, help=\"base learning rate\")\n",
    "parser.add_argument(\"--steps\", type=eval, default=[50, 100], help=\"steps\")\n",
    "parser.add_argument(\"--lr_decay_ratio\", type=float, default=0.1, help=\"lr_decay_ratio\")\n",
    "parser.add_argument(\"--epsilon\", type=float, default=1e-3, help=\"optimizer epsilon\")\n",
    "parser.add_argument(\"--max_grad_norm\", type=int, default=5, help=\"max_grad_norm\")\n",
    "parser.add_argument(\"--use_curriculum_learning\", type=eval, choices=[True, False], default='True', help=\"use_curriculum_learning\")\n",
    "parser.add_argument(\"--cl_decay_steps\", type=int, default=2000, help=\"cl_decay_steps\")\n",
    "parser.add_argument('--test_every_n_epochs', type=int, default=5, help='test_every_n_epochs')\n",
    "parser.add_argument('--gpu', type=int, default=0, help='which gpu to use')\n",
    "parser.add_argument('--num_class', type=int, default=3, help='number of classes')\n",
    "# parser.add_argument('--seed', type=int, default=100, help='random seed.')\n",
    "args = parser.parse_args(args=[]) # use it for jupyter notebook\n",
    "# args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../save/Flu_11_12_MegaCRN_20240615134216\\\\utils.py'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if args.dataset == 'METRLA':\n",
    "    data_path = f'../{args.dataset}/metr-la.h5'\n",
    "    args.num_nodes = 207\n",
    "elif args.dataset == 'PEMSBAY':\n",
    "    data_path = f'../{args.dataset}/pems-bay.h5'\n",
    "    args.num_nodes = 325\n",
    "else:\n",
    "    pass # including more datasets in the future    \n",
    "\n",
    "model_name = 'MegaCRN'\n",
    "timestring = time.strftime('%Y%m%d%H%M%S', time.localtime())\n",
    "path = f'../save/{args.dataset}_{model_name}_{timestring}'\n",
    "logging_path = f'{path}/{model_name}_{timestring}_logging.txt'\n",
    "score_path = f'{path}/{model_name}_{timestring}_scores.txt'\n",
    "epochlog_path = f'{path}/{model_name}_{timestring}_epochlog.txt'\n",
    "modelpt_path = f'{path}/{model_name}_{timestring}.pt'\n",
    "if not os.path.exists(path): os.makedirs(path)\n",
    "shutil.copy2(sys.argv[0], path)\n",
    "shutil.copy2(f'{model_name}.py', path)\n",
    "shutil.copy2('utils.py', path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model MegaCRN \n",
      "dataset Flu_11_12 \n",
      "trainval_ratio 0.8 \n",
      "val_ratio 0.125 \n",
      "num_nodes 56 \n",
      "seq_len 12 \n",
      "horizon 12 \n",
      "input_dim 1 \n",
      "output_dim 1 \n",
      "num_rnn_layers 1 \n",
      "rnn_units 64 \n",
      "max_diffusion_step 3 \n",
      "mem_num 20 \n",
      "mem_dim 64 \n",
      "loss mask_mae_loss \n",
      "separate loss lamb 0.01 \n",
      "compact loss lamb1 0.01 \n",
      "batch_size 64 \n",
      "epochs 2 \n",
      "patience 20 \n",
      "lr 0.01 \n",
      "epsilon 0.001 \n",
      "steps [50, 100] \n",
      "lr_decay_ratio 0.1 \n",
      "use_curriculum_learning True \n"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(level = logging.INFO)\n",
    "class MyFormatter(logging.Formatter):\n",
    "    def format(self, record):\n",
    "        spliter = ' '\n",
    "        record.msg = str(record.msg) + spliter + spliter.join(map(str, record.args))\n",
    "        record.args = tuple() # set empty to args\n",
    "        return super().format(record)\n",
    "formatter = MyFormatter()\n",
    "handler = logging.FileHandler(logging_path, mode='a')\n",
    "handler.setLevel(logging.INFO)\n",
    "handler.setFormatter(formatter)\n",
    "console = logging.StreamHandler()\n",
    "console.setLevel(logging.INFO)\n",
    "console.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "logger.addHandler(console)\n",
    "\n",
    "logger.info('model', model_name)\n",
    "logger.info('dataset', args.dataset)\n",
    "logger.info('trainval_ratio', args.trainval_ratio)\n",
    "logger.info('val_ratio', args.val_ratio)\n",
    "logger.info('num_nodes', args.num_nodes)\n",
    "logger.info('seq_len', args.seq_len)\n",
    "logger.info('horizon', args.horizon)\n",
    "logger.info('input_dim', args.input_dim)\n",
    "logger.info('output_dim', args.output_dim)\n",
    "logger.info('num_rnn_layers', args.num_rnn_layers)\n",
    "logger.info('rnn_units', args.rnn_units)\n",
    "logger.info('max_diffusion_step', args.max_diffusion_step)\n",
    "logger.info('mem_num', args.mem_num)\n",
    "logger.info('mem_dim', args.mem_dim)\n",
    "logger.info('loss', args.loss)\n",
    "logger.info('separate loss lamb', args.lamb)\n",
    "logger.info('compact loss lamb1', args.lamb1)\n",
    "logger.info('batch_size', args.batch_size)\n",
    "logger.info('epochs', args.epochs)\n",
    "logger.info('patience', args.patience)\n",
    "logger.info('lr', args.lr)\n",
    "logger.info('epsilon', args.epsilon)\n",
    "logger.info('steps', args.steps)\n",
    "logger.info('lr_decay_ratio', args.lr_decay_ratio)\n",
    "logger.info('use_curriculum_learning', args.use_curriculum_learning)\n",
    "\n",
    "cpu_num = 1\n",
    "os.environ ['OMP_NUM_THREADS'] = str(cpu_num)\n",
    "os.environ ['OPENBLAS_NUM_THREADS'] = str(cpu_num)\n",
    "os.environ ['MKL_NUM_THREADS'] = str(cpu_num)\n",
    "os.environ ['VECLIB_MAXIMUM_THREADS'] = str(cpu_num)\n",
    "os.environ ['NUMEXPR_NUM_THREADS'] = str(cpu_num)\n",
    "torch.set_num_threads(cpu_num)\n",
    "# device = torch.device(\"cuda:{}\".format(args.gpu)) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Please comment the following three lines for running experiments multiple times.\n",
    "# np.random.seed(args.seed)\n",
    "# torch.manual_seed(args.seed)\n",
    "# if torch.cuda.is_available(): torch.cuda.manual_seed(args.seed)\n",
    "#####################################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = {}\n",
    "for category in ['train', 'val', 'test']:\n",
    "    cat_data = np.load(os.path.join(f'../{args.dataset}', category + '.npz'))\n",
    "    data['x_' + category] = cat_data['x']\n",
    "    data['y_' + category] = cat_data['y']\n",
    "scaler = StandardScaler(mean=data['x_train'][..., 0].mean(), std=data['x_train'][..., 0].std())\n",
    "for category in ['train', 'val', 'test']:\n",
    "    data['x_' + category][..., 0] = scaler.transform(data['x_' + category][..., 0])\n",
    "    data['y_' + category][..., 0] = scaler.transform(data['y_' + category][..., 0])\n",
    "data['train_loader'] = DataLoader(data['x_train'], data['y_train'], args.batch_size, shuffle=True)\n",
    "data['val_loader'] = DataLoader(data['x_val'], data['y_val'], args.batch_size, shuffle=False)\n",
    "data['test_loader'] = DataLoader(data['x_test'], data['y_test'], args.batch_size, shuffle=False)\n",
    "\n",
    "def main():\n",
    "    logger.info(args.dataset, 'training and testing started', time.ctime())\n",
    "    logger.info('train xs.shape, ys.shape', data['x_train'].shape, data['y_train'].shape)\n",
    "    logger.info('val xs.shape, ys.shape', data['x_val'].shape, data['y_val'].shape)\n",
    "    logger.info('test xs.shape, ys.shape', data['x_test'].shape, data['y_test'].shape)\n",
    "    traintest_model()\n",
    "    logger.info(args.dataset, 'training and testing ended', time.ctime())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainable parameter list:  \n",
      "In total: 382721 trainable parameters.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory.Memory torch.Size([20, 64]) 1280\n",
      "memory.Wq torch.Size([64, 64]) 4096\n",
      "memory.We1 torch.Size([56, 20]) 1120\n",
      "memory.We2 torch.Size([56, 20]) 1120\n",
      "encoder.dcrnn_cells.0.gate.weights torch.Size([390, 128]) 49920\n",
      "encoder.dcrnn_cells.0.gate.bias torch.Size([128]) 128\n",
      "encoder.dcrnn_cells.0.update.weights torch.Size([390, 64]) 24960\n",
      "encoder.dcrnn_cells.0.update.bias torch.Size([64]) 64\n",
      "decoder.dcrnn_cells.0.gate.weights torch.Size([780, 256]) 199680\n",
      "decoder.dcrnn_cells.0.gate.bias torch.Size([256]) 256\n",
      "decoder.dcrnn_cells.0.update.weights torch.Size([780, 128]) 99840\n",
      "decoder.dcrnn_cells.0.update.bias torch.Size([128]) 128\n",
      "proj.0.weight torch.Size([1, 128]) 128\n",
      "proj.0.bias torch.Size([1]) 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/2] (8) train_loss: 0.7074, val_loss: 0.0879, lr: 0.010000, 1.8s  \n",
      "Horizon overall: mae: 0.7893, mape: 0.2651, rmse: 1.3085  \n",
      "Horizon 15mins: mae: 0.7886, mape: 0.2576, rmse: 1.3194  \n",
      "Horizon 30mins: mae: 0.8350, mape: 0.2735, rmse: 1.3625  \n",
      "Horizon 60mins: mae: 0.8607, mape: 0.2915, rmse: 1.3764  \n",
      "Epoch [2/2] (16) train_loss: 0.3269, val_loss: 0.0750, lr: 0.010000, 1.6s  \n",
      "Horizon overall: mae: 0.7368, mape: 0.2613, rmse: 1.2045  \n",
      "Horizon 15mins: mae: 0.7417, mape: 0.2581, rmse: 1.2189  \n",
      "Horizon 30mins: mae: 0.7820, mape: 0.2687, rmse: 1.2617  \n",
      "Horizon 60mins: mae: 0.8011, mape: 0.2793, rmse: 1.2807  \n",
      "===================================Best model performance===================================  \n",
      "Horizon overall: mae: 0.7368, mape: 0.2613, rmse: 1.2045  \n",
      "Horizon 15mins: mae: 0.7417, mape: 0.2581, rmse: 1.2189  \n",
      "Horizon 30mins: mae: 0.7820, mape: 0.2687, rmse: 1.2617  \n",
      "Horizon 60mins: mae: 0.8011, mape: 0.2793, rmse: 1.2807  \n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "print_model(model)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, eps=args.epsilon)\n",
    "lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=args.steps, gamma=args.lr_decay_ratio)\n",
    "min_val_loss = float('inf')\n",
    "wait = 0\n",
    "batches_seen = 0\n",
    "for epoch_num in range(args.epochs):\n",
    "    start_time = time.time()\n",
    "    model = model.train()\n",
    "    data_iter = data['train_loader'].get_iterator()\n",
    "    losses = []\n",
    "    for x, y in data_iter:\n",
    "        optimizer.zero_grad()\n",
    "        x, y, ycov = prepare_x_y(x, y)\n",
    "        output, h_att, query, pos, neg = model(x, ycov, y, batches_seen)\n",
    "        y_pred = scaler.inverse_transform(output)\n",
    "        y_true = scaler.inverse_transform(y)\n",
    "        loss1 = masked_mae_loss(y_pred, y_true) # masked_mae_loss(y_pred, y_true)\n",
    "        separate_loss = nn.TripletMarginLoss(margin=1.0)\n",
    "        compact_loss = nn.MSELoss()\n",
    "        loss2 = separate_loss(query, pos.detach(), neg.detach())\n",
    "        loss3 = compact_loss(query, pos.detach())\n",
    "        loss = loss1 + args.lamb * loss2 + args.lamb1 * loss3\n",
    "        losses.append(loss.item())\n",
    "        batches_seen += 1\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm) # gradient clipping - this does it in place\n",
    "        optimizer.step()\n",
    "    train_loss = np.mean(losses)\n",
    "    lr_scheduler.step()\n",
    "    val_loss, _, _ = evaluate(model, 'val')\n",
    "    # if (epoch_num % args.test_every_n_epochs) == args.test_every_n_epochs - 1:\n",
    "    end_time2 = time.time()\n",
    "    message = 'Epoch [{}/{}] ({}) train_loss: {:.4f}, val_loss: {:.4f}, lr: {:.6f}, {:.1f}s'.format(epoch_num + 1, \n",
    "               args.epochs, batches_seen, train_loss, val_loss, optimizer.param_groups[0]['lr'], (end_time2 - start_time))\n",
    "    logger.info(message)\n",
    "    test_loss, _, _ = evaluate(model, 'test')\n",
    "\n",
    "    if val_loss < min_val_loss:\n",
    "        wait = 0\n",
    "        min_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), modelpt_path)\n",
    "        # logger.info('Val loss decrease from {:.4f} to {:.4f}, saving model to pt'.format(min_val_loss, val_loss))\n",
    "    elif val_loss >= min_val_loss:\n",
    "        wait += 1\n",
    "        if wait == args.patience:\n",
    "            logger.info('Early stopping at epoch: %d' % epoch_num)\n",
    "            break\n",
    "\n",
    "logger.info('=' * 35 + 'Best model performance' + '=' * 35)\n",
    "model = get_model()\n",
    "model.load_state_dict(torch.load(modelpt_path))\n",
    "test_loss, _, _ = evaluate(model, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Horizon overall: mae: 0.7368, mape: 0.2613, rmse: 1.2045  \n",
      "Horizon 15mins: mae: 0.7417, mape: 0.2581, rmse: 1.2189  \n",
      "Horizon 30mins: mae: 0.7820, mape: 0.2687, rmse: 1.2617  \n",
      "Horizon 60mins: mae: 0.8011, mape: 0.2793, rmse: 1.2807  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_loss, _, _ = evaluate(model, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model = model.eval()\n",
    "    data_iter =  data['test_loader'].get_iterator()\n",
    "    losses = []\n",
    "    ys_true, ys_pred = [], []\n",
    "    maes, mapes, mses = [], [], []\n",
    "    l_3, m_3, r_3 = [], [], []\n",
    "    l_6, m_6, r_6 = [], [], []\n",
    "    l_12, m_12, r_12 = [], [], []\n",
    "    for x, y in data_iter:\n",
    "        x, y, ycov = prepare_x_y(x, y)\n",
    "        output, h_att, query, pos, neg = model(x, ycov)\n",
    "        y_pred = scaler.inverse_transform(output)\n",
    "        y_true = scaler.inverse_transform(y)\n",
    "        y_true, y_pred = y_true.permute(1, 0, 2, 3), y_pred.permute(1, 0, 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 56, 1])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[0:1].shape #12, 64, 56, 1 (seq_len, batch_size, num_sensor, input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[(0 < predictions) & (predictions <= 0.5)] = 0\n",
    "            predictions[(0.5 < predictions) & (predictions <= 1.5)] = 1\n",
    "            predictions[(1.5 < predictions)] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Y_test = y_true.transpose(1, 2).reshape((-1, 56))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "mze = ((predictions != Y_test).long().sum() / Y_test.ravel().shape[0]).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7760881781578064"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:SLVAE]",
   "language": "python",
   "name": "conda-env-SLVAE-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
