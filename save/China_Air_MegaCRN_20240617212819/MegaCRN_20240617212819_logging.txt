model MegaCRN
dataset China_Air
trainval_ratio 0.5
val_ratio 0.125
num_nodes 10
seq_len 12
horizon 12
input_dim 1
output_dim 1
num_rnn_layers 1
rnn_units 64
max_diffusion_step 3
mem_num 20
mem_dim 64
loss mask_mae_loss
separate loss lamb 0.01
compact loss lamb1 0.01
batch_size 64
epochs 100
patience 20
lr 0.01
epsilon 0.001
steps [50, 100]
lr_decay_ratio 0.1
use_curriculum_learning True
China_Air training and testing started Mon Jun 17 21:28:19 2024
train xs.shape, ys.shape (171, 12, 10, 2) (171, 12, 10, 2)
val xs.shape, ys.shape (0, 12, 10, 2) (0, 12, 10, 2)
test xs.shape, ys.shape (171, 12, 10, 2) (171, 12, 10, 2)
Trainable parameter list: 
In total: 380881 trainable parameters. 
Epoch [1/100] (3) train_loss: 1.1351, val_loss: nan, lr: 0.010000, 1.1s 
Epoch [2/100] (6) train_loss: 0.6794, val_loss: nan, lr: 0.010000, 0.5s 
Epoch [3/100] (9) train_loss: 0.5092, val_loss: nan, lr: 0.010000, 0.5s 
Epoch [4/100] (12) train_loss: 0.4898, val_loss: nan, lr: 0.010000, 0.5s 
Epoch [5/100] (15) train_loss: 0.4539, val_loss: nan, lr: 0.010000, 0.4s 
Epoch [6/100] (18) train_loss: 0.4577, val_loss: nan, lr: 0.010000, 0.4s 
Epoch [7/100] (21) train_loss: 0.4333, val_loss: nan, lr: 0.010000, 0.4s 
Epoch [8/100] (24) train_loss: 0.4163, val_loss: nan, lr: 0.010000, 0.4s 
Epoch [9/100] (27) train_loss: 0.4255, val_loss: nan, lr: 0.010000, 0.4s 
Epoch [10/100] (30) train_loss: 0.4126, val_loss: nan, lr: 0.010000, 0.5s 
Epoch [11/100] (33) train_loss: 0.4122, val_loss: nan, lr: 0.010000, 0.5s 
Epoch [12/100] (36) train_loss: 0.4151, val_loss: nan, lr: 0.010000, 0.4s 
Epoch [13/100] (39) train_loss: 0.4114, val_loss: nan, lr: 0.010000, 0.5s 
Epoch [14/100] (42) train_loss: 0.4091, val_loss: nan, lr: 0.010000, 0.5s 
Epoch [15/100] (45) train_loss: 0.4015, val_loss: nan, lr: 0.010000, 0.5s 
Epoch [16/100] (48) train_loss: 0.4079, val_loss: nan, lr: 0.010000, 0.4s 
Epoch [17/100] (51) train_loss: 0.4071, val_loss: nan, lr: 0.010000, 0.4s 
Epoch [18/100] (54) train_loss: 0.3837, val_loss: nan, lr: 0.010000, 0.5s 
Epoch [19/100] (57) train_loss: 0.3817, val_loss: nan, lr: 0.010000, 0.5s 
Epoch [20/100] (60) train_loss: 0.3711, val_loss: nan, lr: 0.010000, 0.4s 
Epoch [21/100] (63) train_loss: 0.3678, val_loss: nan, lr: 0.010000, 0.4s 
Epoch [22/100] (66) train_loss: 0.3683, val_loss: nan, lr: 0.010000, 0.5s 
Epoch [23/100] (69) train_loss: 0.3548, val_loss: nan, lr: 0.010000, 0.5s 
Epoch [24/100] (72) train_loss: 0.3592, val_loss: nan, lr: 0.010000, 0.5s 
Epoch [25/100] (75) train_loss: 0.3843, val_loss: nan, lr: 0.010000, 0.6s 
Epoch [26/100] (78) train_loss: 0.4016, val_loss: nan, lr: 0.010000, 0.5s 
Epoch [27/100] (81) train_loss: 0.3619, val_loss: nan, lr: 0.010000, 0.5s 
Epoch [28/100] (84) train_loss: 0.3557, val_loss: nan, lr: 0.010000, 0.5s 
Epoch [29/100] (87) train_loss: 0.3129, val_loss: nan, lr: 0.010000, 0.4s 
Epoch [30/100] (90) train_loss: 0.3174, val_loss: nan, lr: 0.010000, 0.4s 
Epoch [31/100] (93) train_loss: 0.3274, val_loss: nan, lr: 0.010000, 0.5s 
Epoch [32/100] (96) train_loss: 0.3639, val_loss: nan, lr: 0.010000, 0.4s 
Epoch [33/100] (99) train_loss: 0.3346, val_loss: nan, lr: 0.010000, 0.5s 
Epoch [34/100] (102) train_loss: 0.3322, val_loss: nan, lr: 0.010000, 0.5s 
Epoch [35/100] (105) train_loss: 0.3146, val_loss: nan, lr: 0.010000, 0.5s 
Epoch [36/100] (108) train_loss: 0.2907, val_loss: nan, lr: 0.010000, 0.5s 
Epoch [37/100] (111) train_loss: 0.2974, val_loss: nan, lr: 0.010000, 0.4s 
Epoch [38/100] (114) train_loss: 0.2914, val_loss: nan, lr: 0.010000, 0.4s 
Epoch [39/100] (117) train_loss: 0.2922, val_loss: nan, lr: 0.010000, 0.5s 
Epoch [40/100] (120) train_loss: 0.2885, val_loss: nan, lr: 0.010000, 0.4s 
Epoch [41/100] (123) train_loss: 0.2877, val_loss: nan, lr: 0.010000, 0.4s 
Epoch [42/100] (126) train_loss: 0.2940, val_loss: nan, lr: 0.010000, 0.4s 
Epoch [43/100] (129) train_loss: 0.3222, val_loss: nan, lr: 0.010000, 0.5s 
Epoch [44/100] (132) train_loss: 0.2927, val_loss: nan, lr: 0.010000, 0.5s 
Epoch [45/100] (135) train_loss: 0.2882, val_loss: nan, lr: 0.010000, 0.5s 
Epoch [46/100] (138) train_loss: 0.3009, val_loss: nan, lr: 0.010000, 0.4s 
Epoch [47/100] (141) train_loss: 0.3127, val_loss: nan, lr: 0.010000, 0.5s 
Epoch [48/100] (144) train_loss: 0.3804, val_loss: nan, lr: 0.010000, 0.4s 
Epoch [49/100] (147) train_loss: 0.3158, val_loss: nan, lr: 0.010000, 0.4s 
Epoch [50/100] (150) train_loss: 0.3031, val_loss: nan, lr: 0.001000, 0.4s 
Epoch [51/100] (153) train_loss: 0.2899, val_loss: nan, lr: 0.001000, 0.4s 
Epoch [52/100] (156) train_loss: 0.2884, val_loss: nan, lr: 0.001000, 0.5s 
Epoch [53/100] (159) train_loss: 0.2809, val_loss: nan, lr: 0.001000, 0.4s 
Epoch [54/100] (162) train_loss: 0.2805, val_loss: nan, lr: 0.001000, 0.4s 
Epoch [55/100] (165) train_loss: 0.2783, val_loss: nan, lr: 0.001000, 0.5s 
Epoch [56/100] (168) train_loss: 0.2751, val_loss: nan, lr: 0.001000, 0.5s 
Epoch [57/100] (171) train_loss: 0.2744, val_loss: nan, lr: 0.001000, 0.4s 
Epoch [58/100] (174) train_loss: 0.2727, val_loss: nan, lr: 0.001000, 0.5s 
Epoch [59/100] (177) train_loss: 0.2715, val_loss: nan, lr: 0.001000, 0.4s 
Epoch [60/100] (180) train_loss: 0.2706, val_loss: nan, lr: 0.001000, 0.4s 
Epoch [61/100] (183) train_loss: 0.2699, val_loss: nan, lr: 0.001000, 0.5s 
Epoch [62/100] (186) train_loss: 0.2692, val_loss: nan, lr: 0.001000, 0.5s 
Epoch [63/100] (189) train_loss: 0.2690, val_loss: nan, lr: 0.001000, 0.5s 
Epoch [64/100] (192) train_loss: 0.2691, val_loss: nan, lr: 0.001000, 0.5s 
Epoch [65/100] (195) train_loss: 0.2688, val_loss: nan, lr: 0.001000, 0.5s 
Epoch [66/100] (198) train_loss: 0.2683, val_loss: nan, lr: 0.001000, 0.5s 
Epoch [67/100] (201) train_loss: 0.2680, val_loss: nan, lr: 0.001000, 0.5s 
Epoch [68/100] (204) train_loss: 0.2681, val_loss: nan, lr: 0.001000, 0.5s 
Epoch [69/100] (207) train_loss: 0.2681, val_loss: nan, lr: 0.001000, 0.5s 
Epoch [70/100] (210) train_loss: 0.2677, val_loss: nan, lr: 0.001000, 0.5s 
Epoch [71/100] (213) train_loss: 0.2676, val_loss: nan, lr: 0.001000, 0.4s 
Epoch [72/100] (216) train_loss: 0.2675, val_loss: nan, lr: 0.001000, 0.4s 
Epoch [73/100] (219) train_loss: 0.2675, val_loss: nan, lr: 0.001000, 0.4s 
Epoch [74/100] (222) train_loss: 0.2673, val_loss: nan, lr: 0.001000, 0.5s 
Epoch [75/100] (225) train_loss: 0.2672, val_loss: nan, lr: 0.001000, 0.5s 
Epoch [76/100] (228) train_loss: 0.2673, val_loss: nan, lr: 0.001000, 0.4s 
Epoch [77/100] (231) train_loss: 0.2672, val_loss: nan, lr: 0.001000, 0.4s 
Epoch [78/100] (234) train_loss: 0.2670, val_loss: nan, lr: 0.001000, 0.5s 
Epoch [79/100] (237) train_loss: 0.2668, val_loss: nan, lr: 0.001000, 0.5s 
Epoch [80/100] (240) train_loss: 0.2664, val_loss: nan, lr: 0.001000, 0.4s 
Epoch [81/100] (243) train_loss: 0.2665, val_loss: nan, lr: 0.001000, 0.5s 
Epoch [82/100] (246) train_loss: 0.2666, val_loss: nan, lr: 0.001000, 0.4s 
Epoch [83/100] (249) train_loss: 0.2677, val_loss: nan, lr: 0.001000, 0.5s 
Epoch [84/100] (252) train_loss: 0.2687, val_loss: nan, lr: 0.001000, 0.4s 
Epoch [85/100] (255) train_loss: 0.2673, val_loss: nan, lr: 0.001000, 0.5s 
Epoch [86/100] (258) train_loss: 0.2673, val_loss: nan, lr: 0.001000, 0.4s 
Epoch [87/100] (261) train_loss: 0.2685, val_loss: nan, lr: 0.001000, 0.5s 
Epoch [88/100] (264) train_loss: 0.2676, val_loss: nan, lr: 0.001000, 0.4s 
Epoch [89/100] (267) train_loss: 0.2673, val_loss: nan, lr: 0.001000, 0.4s 
Epoch [90/100] (270) train_loss: 0.2677, val_loss: nan, lr: 0.001000, 0.4s 
Epoch [91/100] (273) train_loss: 0.2678, val_loss: nan, lr: 0.001000, 0.4s 
Epoch [92/100] (276) train_loss: 0.2671, val_loss: nan, lr: 0.001000, 0.5s 
Epoch [93/100] (279) train_loss: 0.2667, val_loss: nan, lr: 0.001000, 0.5s 
Epoch [94/100] (282) train_loss: 0.2660, val_loss: nan, lr: 0.001000, 0.5s 
Epoch [95/100] (285) train_loss: 0.2667, val_loss: nan, lr: 0.001000, 0.5s 
Epoch [96/100] (288) train_loss: 0.2661, val_loss: nan, lr: 0.001000, 0.4s 
Epoch [97/100] (291) train_loss: 0.2660, val_loss: nan, lr: 0.001000, 0.5s 
Epoch [98/100] (294) train_loss: 0.2661, val_loss: nan, lr: 0.001000, 0.4s 
Epoch [99/100] (297) train_loss: 0.2661, val_loss: nan, lr: 0.001000, 0.4s 
Epoch [100/100] (300) train_loss: 0.2658, val_loss: nan, lr: 0.000100, 0.4s 
===================================Best model performance=================================== 
Horizon overall: mze: 0.7249, mae: 0.9631 
Horizon 1 day: mze: 0.7302, mae: 0.9615, precision: 0.2422, recall: 0.4224, f1: 0.2472 
Horizon 3 day: mze: 0.7333, mae: 0.9656, precision: 0.2569, recall: 0.4222, f1: 0.2517 
Horizon 5 day: mze: 0.7099, mae: 0.9370, precision: 0.2623, recall: 0.4394, f1: 0.2702 
Horizon 7 day: mze: 0.7453, mae: 0.9953, precision: 0.2433, recall: 0.4253, f1: 0.2416 
China_Air training and testing ended Mon Jun 17 21:29:08 2024
